# Import all required modules# Written in VS2015from bs4 import BeautifulSoupimport requestsimport jsonimport redef find_between( s, first, last ):    try:        start = s.index( first ) + len( first )        end = s.index( last, start )        return s[start:end]    except ValueError:        return ""# Declare Raw Datacompany = "scan-one"baseurl = "http://www.scan-one.com"url = baseurl + "/About_Us/Careers.aspx"#print(url) ##testing!!!jobs = []# Establish link with webpage to scrape.r = requests.get(url)try:    r.raise_for_status()except Exception as exc:    print('there was a problem: %s' % (exc))data = r.textsoup = BeautifulSoup(data, 'html.parser')# 1 - examine HTML that is returned by BS4# this is actually HTML - use this to debug elements; you'll see embedded jsonfor links in soup.find_all('json-source'): # maybe there is a better way to extract json-source??    #print links    content = find_between(str(links), 'json-source html-fields="wideTiles.content" source="','">') # maybe there is a better way to extract JSON source?    if content is not None:        try:            cleaned_json = content.replace('\r\n', '').replace('&quot;', '"').replace('\\', '').replace(r'\"(.+?)\"', '')#.replace('"', '\\"')            print cleaned_json # TODO: string has still double quotes within content which makes it invalid. Need to write a regex to escape double quotes            jobs = json.loads(str(cleaned_json))            print jobs            # TODO; once JSON is valid, iterate through and extract the information            '''            for job in jobs:                jobLocation = None                jobTitle = job['headline']                applicationLink = job['url']                job = {                        'ApplicationLink': applicationLink,                        'Company': company,                        'DatePosted': '',                        'Experience': '',                        'Hours': '',                        'JobID': '',                        'JobTitle': jobTitle,                        'LanguagesUsed': '',                        #'Location': jobLocation, #company only exists in PDX, don't need to search for location.                        'Salary': ''                    }                jobs.append(job)                '''        except Exception as e:            print 'SKIPPING: Error processing JSON: %s' % str(e)# Convert jobs list of dictionaries to .json file.with open(company + '.json', 'w') as outfile:    json.dump(jobs, outfile)